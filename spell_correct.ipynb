{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPELL CORRECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18210980078 张屹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edittype(word,error):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    word,error = word.lower(),error.lower()\n",
    "    for i in range(len(word)):\n",
    "        if word == error[1:]:\n",
    "            return error[0]+'|<s>', 'ins'\n",
    "        if word[1:] == error:\n",
    "            return '|'+word[0], 'del'\n",
    "        if i >= len(error):\n",
    "            return word[i]+'|'+error[i-1]+word[i], 'del'\n",
    "        elif word[i] != error[i]:\n",
    "            print(word,error)\n",
    "            if word in [error[:i]+k+error[i:] for k in letters]:\n",
    "                return error[i-1]+'|'+error[i-1]+word[i], 'del'\n",
    "            elif word in [error[:i]+k+error[i+1:] for k in letters]:\n",
    "                return error[i]+'|'+word[i], 'sub'\n",
    "            elif word == error[:i]+error[i+1:] or word == error[:-1]:\n",
    "                return word[i-1]+error[i]+'|'+word[i-1], 'ins'\n",
    "            elif word[i]+ word[i+1] == error[i+1]+error[i]:\n",
    "                return word[i+1]+word[i]+'|'+word[i]+word[i+1], 'trans'\n",
    "    if len(word)<len(error):\n",
    "        return word[-1]+error[-1]+'|'+word[-1], 'ins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sam asm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('as|sa', 'trans')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edittype('sam','asm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editdistance(s1, s2): \n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    "     \n",
    "    matrix = [[i+j for j in range(len2 + 1)] for i in range(len1 + 1)]\n",
    "#     print(matrix)\n",
    "    for row in range(len1):\n",
    "        for col in range(len2):\n",
    "            comp = [matrix[row+1][col]+1, matrix[row][col+1]+1]\n",
    "             \n",
    "            if s1[row] == s2[col]:\n",
    "                comp.append(matrix[row][col])\n",
    "            else:\n",
    "                comp.append(matrix[row][col]+1)\n",
    "             \n",
    "            if row > 0 and col > 0:\n",
    "                if s1[row] == s2[col-1] and s1[row-1] == s2[col]:\n",
    "                    comp.append(matrix[row-1][col-1]+1)\n",
    "                     \n",
    "            matrix[row+1][col+1] = min(comp)\n",
    "             \n",
    "    return matrix[len1][len2],matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [3, 4, 5, 6, 7, 8, 9, 10, 11, 12], [4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [7, 8, 9, 10, 11, 12, 13, 14, 15, 16], [8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [9, 10, 11, 12, 13, 14, 15, 16, 17, 18]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "  [1, 1, 2, 3, 4, 5, 6, 6, 7, 8],\n",
       "  [2, 2, 2, 3, 4, 5, 6, 7, 7, 7],\n",
       "  [3, 3, 3, 3, 4, 5, 5, 6, 7, 8],\n",
       "  [4, 3, 4, 3, 4, 5, 6, 6, 7, 8],\n",
       "  [5, 4, 4, 4, 4, 5, 6, 7, 7, 7],\n",
       "  [6, 5, 5, 5, 5, 5, 5, 6, 7, 8],\n",
       "  [7, 6, 6, 6, 6, 6, 6, 5, 6, 7],\n",
       "  [8, 7, 7, 7, 7, 7, 7, 6, 5, 6],\n",
       "  [9, 8, 8, 8, 8, 8, 8, 7, 6, 5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "editdistance('intention','execution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    return set(word for word in words if word in VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_type(candidate, word):\n",
    "    wrong_type = edittype(word,candidate)\n",
    "    if wrong_type[0] in EDIT_P.keys():\n",
    "        return EDIT_P[wrong_type[0]]\n",
    "    else:\n",
    "        return -100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob1(candidate, word, sentence):\n",
    "    if candidate in UNIGRAM.keys():\n",
    "        p_unigram = UNIGRAM[candidate]\n",
    "    else:\n",
    "        p_unigram = -100000\n",
    "    return p_unigram + p_type(candidate, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIGRAM_p(s):\n",
    "    if s in BIGRAM.keys():\n",
    "        return BIGRAM[s]\n",
    "    else:\n",
    "        return -100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob2(candidate, j, sentence):\n",
    "    if j==0:\n",
    "        return BIGRAM_p(candidate+' '+sentence[j+1].lower())\n",
    "    elif j==len(sentence)-1:\n",
    "        return BIGRAM_p((sentence[j-2].lower()+' '+candidate))\n",
    "    else:\n",
    "        return BIGRAM_p(candidate+' '+sentence[j+1].lower()) + BIGRAM_p(sentence[j-1].lower()+' '+candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_word_correct(sentence):\n",
    "    wrong = 0\n",
    "    for j in range(len(sentence)):\n",
    "        word = sentence[j]\n",
    "        if bool(re.search(r\"[\\d.,/'-]\", word)) or word.lower() in VOCAB:\n",
    "            continue\n",
    "        # edit distance = 1\n",
    "        word_lower = word.lower()\n",
    "        candidates = known(edits1(word_lower))\n",
    "        # edit distance = 2\n",
    "        if len(candidates)==0:\n",
    "            candidates = known(edits2(word_lower))\n",
    "        p_flag = -200000\n",
    "        right = word\n",
    "        for candidate in candidates:\n",
    "            # unigram prob\n",
    "#             p = prob1(candidate, word, sentence) #* prob2(error, candidate)\n",
    "            p = prob2(candidate,j, sentence) # + p_type(candidate,word_lower) #channel model for edits1\n",
    "            if p > p_flag:\n",
    "                p_flag = p\n",
    "                right = candidate\n",
    "        if not word.islower():\n",
    "            flag = 0\n",
    "            for each in word:\n",
    "                flag += int(each.isupper())\n",
    "            if flag == 1:\n",
    "                right = right[0].upper()+right[1:]\n",
    "            else:\n",
    "                right = right.upper()\n",
    "        sentence[j] = right # to do supper letters\n",
    "        wrong += 1\n",
    "    return wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_word_correct(sentence):\n",
    "    for j in range(len(sentence)):\n",
    "        word = sentence[j]\n",
    "        if bool(re.search(r\"[\\d.,/'-]\", word)):\n",
    "            continue\n",
    "        # edit distance = 1\n",
    "        word_lower = word.lower()\n",
    "        candidates = known(edits1(word_lower))\n",
    "        if len(candidates)==0:\n",
    "            candidates = known(edits2(word_lower))\n",
    "#         candidates = candidates|known(edits2(word_lower))\n",
    "        candidates.add(word_lower)\n",
    "        p_flag = -200000\n",
    "        right = word_lower\n",
    "        for candidate in candidates:\n",
    "            # bigram prob\n",
    "            p = prob2(candidate,j, sentence)\n",
    "            if p > p_flag:\n",
    "                p_flag = p\n",
    "                right = candidate\n",
    "#         print(candidates,right)\n",
    "        if right == word_lower:\n",
    "            continue\n",
    "        if word != word_lower:\n",
    "            flag = 0\n",
    "            for each in word:\n",
    "                flag += int(each.isupper())\n",
    "            if flag == 1:\n",
    "                right = right[0].upper()+right[1:]\n",
    "            else:\n",
    "                right = right.upper()\n",
    "        sentence[j] = right # to do supper letters\n",
    "#         print(j,sentence,word,right)\n",
    "        if right.lower() != word_lower:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_correct(sentences_origin,wrong_words_num, VOCAB):\n",
    "    sentences = deepcopy(sentences_origin)\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        wrong = non_word_correct(sentence)\n",
    "        if wrong < wrong_words_num[i]:\n",
    "            real_word_correct(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'r') as f:\n",
    "    VOCAB = [re.sub('[0-9,.!“”‘’/\\—\\–\\-\\'\\\"]', '', line.strip()) for line in f.readlines()]\n",
    "VOCAB = set([each.lower() for each in VOCAB if each!=''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testdata.txt','r') as f:\n",
    "    sentences = f.read().split('\\n')\n",
    "wrong_words_num = [int(each.split('\\t')[1]) for each in sentences]\n",
    "sentences_origin = [nltk.word_tokenize(each.split('\\t')[2]) for each in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('channel_model.txt',encoding='UTF-8') as f:\n",
    "    EDIT_P = {}\n",
    "    l = f.readlines()\n",
    "#     print(l[0:3])\n",
    "    for each in l:\n",
    "        each = each.strip().split('\\t')\n",
    "#         print(each)\n",
    "        EDIT_P[each[0]] = np.log(int(each[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(file, n):\n",
    "    with open(file) as f:\n",
    "        p = f.read()\n",
    "    p = [[re.sub('[,.!“”‘’\\—\\–\\'\\\"]', '', word).lower() for word in sentence.split()] for sentence in nltk.sent_tokenize(p)]\n",
    "    output = {}\n",
    "    for l in p:\n",
    "        for i in range(len(l)-n+1):\n",
    "            temp = \" \".join(l[i:i+n])\n",
    "            if temp not in output:\n",
    "                output[temp] = 0\n",
    "            output[temp] += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'ans.txt'\n",
    "UNIGRAM = ngram(file,1)\n",
    "SUM = sum(UNIGRAM.values())\n",
    "for each in UNIGRAM:\n",
    "    UNIGRAM[each] = np.log(UNIGRAM[each]/SUM)\n",
    "BIGRAM = ngram(file,2)\n",
    "SUM = sum(BIGRAM.values())\n",
    "for each in BIGRAM:\n",
    "    BIGRAM[each] = np.log(BIGRAM[each]/SUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_correct = word_correct(sentences_origin, wrong_words_num, VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'result.txt'\n",
    "with open(file,'w') as f:\n",
    "    for i,each in enumerate(sentences_correct):\n",
    "        sentence = ' '.join(sentences_correct[i])\n",
    "        f.write(str(i+1)+'\\t'+sentence+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none word edits1 candidates unigram 40% \n",
    "# edits2 candidates 45.7%\n",
    "# upper letter 85.8%\n",
    "# real word edits1  bigram 89.2%\n",
    "# none word bigram 93.1%\n",
    "# to do: \n",
    "# p_type \n",
    "# UNIGRAM for a larger corpus\n",
    "# count BIGRAM Prob\n",
    "# real word error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
